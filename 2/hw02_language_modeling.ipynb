{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
    "\n",
    "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import string\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu device is available\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('{} device is available'.format(device))\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  256k  100  256k    0     0   389k      0 --:--:-- --:--:-- --:--:--  389k\n"
     ]
    }
   ],
   "source": [
    "# Загрузка файла с помощью curl\n",
    "!curl -o onegin.txt https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPenWOy01Ooa",
    "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
   },
   "source": [
    "#### 1. Загрузка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt%%!\n",
    "    \n",
    "with open('onegin.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'а\\nбокала полного вина,\\nкто не дочел ее романа\\nи вдруг умел расстаться с ним,\\nкак я с онегиным моим.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-100:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQYpmGfR_gJ8"
   },
   "source": [
    "#### 2. Построение словаря и предобработка текста\n",
    "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "tokens = sorted(set(text.lower())) + ['<sos>']\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "assert num_tokens == 84, \"Check the tokenization process\"\n",
    "\n",
    "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
    "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
    "\n",
    "print(\"Seems fine!\")\n",
    "\n",
    "\n",
    "text_encoded = [token_to_idx[x] for x in text]\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
    "\n",
    "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
    "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
    "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
    "\n",
    "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
    "\n",
    "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "batch_size = 256\n",
    "seq_length = 100\n",
    "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
    "\n",
    "def generate_chunk():\n",
    "    global text_encoded, start_column, batch_size, seq_length\n",
    "\n",
    "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
    "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
    "    yield np.hstack((start_column, data))\n",
    "# __________end of block__________    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример батча:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 53, 62, ..., 63, 53, 66],\n",
       "       [83, 45, 66, ..., 55, 53,  1],\n",
       "       [83, 57, 59, ..., 52, 53, 57],\n",
       "       ...,\n",
       "       [83,  1, 59, ...,  7,  0, 66],\n",
       "       [83, 56, 45, ..., 61, 63, 58],\n",
       "       [83, 72, 50, ..., 76, 63, 73]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(generate_chunk())\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, num_tokens, hidden_size, n_layers=2, dropout=0.3):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_tokens, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_tokens)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 4.437472820281982\n",
      "Epoch: 1, Step: 0, Loss: 4.186212539672852\n",
      "Epoch: 2, Step: 0, Loss: 3.7909586429595947\n",
      "Epoch: 3, Step: 0, Loss: 3.4447176456451416\n",
      "Epoch: 4, Step: 0, Loss: 3.3681161403656006\n",
      "Epoch: 5, Step: 0, Loss: 3.327181816101074\n",
      "Epoch: 6, Step: 0, Loss: 3.266806125640869\n",
      "Epoch: 7, Step: 0, Loss: 3.241899013519287\n",
      "Epoch: 8, Step: 0, Loss: 3.2163071632385254\n",
      "Epoch: 9, Step: 0, Loss: 3.1585259437561035\n",
      "Epoch: 10, Step: 0, Loss: 3.114703416824341\n",
      "Epoch: 11, Step: 0, Loss: 3.0640954971313477\n",
      "Epoch: 12, Step: 0, Loss: 3.0021677017211914\n",
      "Epoch: 13, Step: 0, Loss: 2.9576408863067627\n",
      "Epoch: 14, Step: 0, Loss: 2.9278857707977295\n",
      "Epoch: 15, Step: 0, Loss: 2.89848256111145\n",
      "Epoch: 16, Step: 0, Loss: 2.858078718185425\n",
      "Epoch: 17, Step: 0, Loss: 2.8101871013641357\n",
      "Epoch: 18, Step: 0, Loss: 2.7992825508117676\n",
      "Epoch: 19, Step: 0, Loss: 2.7694191932678223\n",
      "Epoch: 20, Step: 0, Loss: 2.7432570457458496\n",
      "Epoch: 21, Step: 0, Loss: 2.726238489151001\n",
      "Epoch: 22, Step: 0, Loss: 2.714766263961792\n",
      "Epoch: 23, Step: 0, Loss: 2.687544822692871\n",
      "Epoch: 24, Step: 0, Loss: 2.6882498264312744\n",
      "Epoch: 25, Step: 0, Loss: 2.6690595149993896\n",
      "Epoch: 26, Step: 0, Loss: 2.6330959796905518\n",
      "Epoch: 27, Step: 0, Loss: 2.6154537200927734\n",
      "Epoch: 28, Step: 0, Loss: 2.6292495727539062\n",
      "Epoch: 29, Step: 0, Loss: 2.6099157333374023\n",
      "Epoch: 30, Step: 0, Loss: 2.588907241821289\n",
      "Epoch: 31, Step: 0, Loss: 2.5784032344818115\n",
      "Epoch: 32, Step: 0, Loss: 2.5690107345581055\n",
      "Epoch: 33, Step: 0, Loss: 2.569833993911743\n",
      "Epoch: 34, Step: 0, Loss: 2.5452921390533447\n",
      "Epoch: 35, Step: 0, Loss: 2.54780650138855\n",
      "Epoch: 36, Step: 0, Loss: 2.531125545501709\n",
      "Epoch: 37, Step: 0, Loss: 2.536190986633301\n",
      "Epoch: 38, Step: 0, Loss: 2.5237951278686523\n",
      "Epoch: 39, Step: 0, Loss: 2.4969449043273926\n",
      "Epoch: 40, Step: 0, Loss: 2.504312038421631\n",
      "Epoch: 41, Step: 0, Loss: 2.492377281188965\n",
      "Epoch: 42, Step: 0, Loss: 2.475163698196411\n",
      "Epoch: 43, Step: 0, Loss: 2.4681174755096436\n",
      "Epoch: 44, Step: 0, Loss: 2.474113702774048\n",
      "Epoch: 45, Step: 0, Loss: 2.4695746898651123\n",
      "Epoch: 46, Step: 0, Loss: 2.445988655090332\n",
      "Epoch: 47, Step: 0, Loss: 2.4742352962493896\n",
      "Epoch: 48, Step: 0, Loss: 2.4460761547088623\n",
      "Epoch: 49, Step: 0, Loss: 2.434852123260498\n",
      "Epoch: 50, Step: 0, Loss: 2.4317495822906494\n",
      "Epoch: 51, Step: 0, Loss: 2.4257686138153076\n",
      "Epoch: 52, Step: 0, Loss: 2.4103918075561523\n",
      "Epoch: 53, Step: 0, Loss: 2.3921713829040527\n",
      "Epoch: 54, Step: 0, Loss: 2.4117093086242676\n",
      "Epoch: 55, Step: 0, Loss: 2.394460439682007\n",
      "Epoch: 56, Step: 0, Loss: 2.3829898834228516\n",
      "Epoch: 57, Step: 0, Loss: 2.3656108379364014\n",
      "Epoch: 58, Step: 0, Loss: 2.3873343467712402\n",
      "Epoch: 59, Step: 0, Loss: 2.3683559894561768\n",
      "Epoch: 60, Step: 0, Loss: 2.3764848709106445\n",
      "Epoch: 61, Step: 0, Loss: 2.3415746688842773\n",
      "Epoch: 62, Step: 0, Loss: 2.335256576538086\n",
      "Epoch: 63, Step: 0, Loss: 2.345766067504883\n",
      "Epoch: 64, Step: 0, Loss: 2.36129093170166\n",
      "Epoch: 65, Step: 0, Loss: 2.3329384326934814\n",
      "Epoch: 66, Step: 0, Loss: 2.3418848514556885\n",
      "Epoch: 67, Step: 0, Loss: 2.331287145614624\n",
      "Epoch: 68, Step: 0, Loss: 2.3048110008239746\n",
      "Epoch: 69, Step: 0, Loss: 2.289461135864258\n",
      "Epoch: 70, Step: 0, Loss: 2.3077542781829834\n",
      "Epoch: 71, Step: 0, Loss: 2.3023841381073\n",
      "Epoch: 72, Step: 0, Loss: 2.3117318153381348\n",
      "Epoch: 73, Step: 0, Loss: 2.2827682495117188\n",
      "Epoch: 74, Step: 0, Loss: 2.3038177490234375\n",
      "Epoch: 75, Step: 0, Loss: 2.2485997676849365\n",
      "Epoch: 76, Step: 0, Loss: 2.283569574356079\n",
      "Epoch: 77, Step: 0, Loss: 2.255347728729248\n",
      "Epoch: 78, Step: 0, Loss: 2.245361089706421\n",
      "Epoch: 79, Step: 0, Loss: 2.2243032455444336\n",
      "Epoch: 80, Step: 0, Loss: 2.2537965774536133\n",
      "Epoch: 81, Step: 0, Loss: 2.245004415512085\n",
      "Epoch: 82, Step: 0, Loss: 2.2426528930664062\n",
      "Epoch: 83, Step: 0, Loss: 2.258665084838867\n",
      "Epoch: 84, Step: 0, Loss: 2.2113120555877686\n",
      "Epoch: 85, Step: 0, Loss: 2.2265396118164062\n",
      "Epoch: 86, Step: 0, Loss: 2.216970920562744\n",
      "Epoch: 87, Step: 0, Loss: 2.214656114578247\n",
      "Epoch: 88, Step: 0, Loss: 2.18786358833313\n",
      "Epoch: 89, Step: 0, Loss: 2.20642352104187\n",
      "Epoch: 90, Step: 0, Loss: 2.178081512451172\n",
      "Epoch: 91, Step: 0, Loss: 2.224907636642456\n",
      "Epoch: 92, Step: 0, Loss: 2.1867504119873047\n",
      "Epoch: 93, Step: 0, Loss: 2.1587820053100586\n",
      "Epoch: 94, Step: 0, Loss: 2.1675314903259277\n",
      "Epoch: 95, Step: 0, Loss: 2.176731824874878\n",
      "Epoch: 96, Step: 0, Loss: 2.192042589187622\n",
      "Epoch: 97, Step: 0, Loss: 2.129065752029419\n",
      "Epoch: 98, Step: 0, Loss: 2.195793628692627\n",
      "Epoch: 99, Step: 0, Loss: 2.1733832359313965\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "num_tokens = len(tokens)\n",
    "dropout = 0.3\n",
    "\n",
    "model = CharRNN(num_tokens, hidden_size, n_layers, dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, num_epochs=100, batch_size=256, seq_length=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for i, batch in enumerate(generate_chunk()):\n",
    "            batch = torch.tensor(batch, dtype=torch.long)\n",
    "            inputs, targets = batch[:, :-1], batch[:, 1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            loss = criterion(output.view(-1, num_tokens), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Step: {i}, Loss: {loss.item()}')\n",
    "    return losses\n",
    "\n",
    "losses = train(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
    "    if seed_phrase is not None:\n",
    "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
    "    else: \n",
    "        x_sequence = [token_to_idx['<sos>']]\n",
    "\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
    "    hidden = char_rnn.init_hidden(1)\n",
    "\n",
    "    generated_text = seed_phrase or \"\"\n",
    "    for _ in range(max_length - len(generated_text)):\n",
    "        output, hidden = char_rnn(x_sequence, hidden)\n",
    "        output = output[:, -1, :] / temperature\n",
    "        probabilities = torch.softmax(output, dim=1).data.cpu().numpy().ravel()\n",
    "        next_token = np.random.choice(num_tokens, p=probabilities)\n",
    "        \n",
    "        x_sequence = torch.cat([x_sequence, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        generated_text += idx_to_token[next_token]\n",
    "    return generated_text.replace('<sos>', '')\n",
    "\n",
    "# Генерация примеров\n",
    "seed_phrase = ' мой дядя самых честных правил'\n",
    "generated_phrases = [\n",
    "    generate_sample(\n",
    "        model,\n",
    "        seed_phrase,\n",
    "        max_length=500,\n",
    "        temperature=0.8  # Экспериментируйте с температурой для лучшего качества\n",
    "    )\n",
    "    for _ in range(10)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " мой дядя самых честных правились,\n",
      "а стра збостные перосювичной\n",
      "остнею влов,\n",
      "то токноворого поры, на взоренье одна был мовес кут никоньта\n",
      "прожие сердце шлег одный праровует,\n",
      "не пойней нески древала\n",
      "их кашно перезнильки белясь негий черская,\n",
      "стах нашет обыла ей бора\n",
      "посел и, полчили,\n",
      "свою ильмо, свезы,\n",
      "лыва, зны двура жисненсосела раздунным…\n",
      "ывен траний назверего.\n",
      "во под мечтво нас попрелпять,\n",
      "предлечал будно встердет она уж кати метел;\n",
      "и ес их надрушах оная прод,\n",
      "в седельный ж нердет ее млень,\n",
      "н\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
    "\n",
    "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_phrase = ' мой дядя самых честных правил'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "import json\n",
    "if 'generated_phrases' not in locals():\n",
    "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
    "\n",
    "for phrase in generated_phrases:\n",
    "\n",
    "    if not isinstance(phrase, str):\n",
    "        raise ValueError(\"The generated phrase should be a string\")\n",
    "\n",
    "    if len(phrase) != 500:\n",
    "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
    "\n",
    "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
    "    \n",
    "\n",
    "submission_dict = {\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'generated_phrases': generated_phrases\n",
    "}\n",
    "\n",
    "with open('submission_dict.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "new_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
